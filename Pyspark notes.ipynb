{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mkl-service in c:\\anaconda\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: six in c:\\anaconda\\lib\\site-packages (from mkl-service) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mkl-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001ED61122C70>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName(\"prajwal\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"C:\\\\my_sql_jar\\\\mysql-connector-java-8.0.26.jar\") \\\n",
    "    .getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|              _c0|                _c1|  _c2|\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"false\")\\\n",
    "            .option(\"inferschema\", \"false\")\\\n",
    "            .option(\"mode\", \"FAILFAST\")\\\n",
    "            .load(\"flight_data.csv\")\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df_header = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferschema\", \"false\")\\\n",
    "            .option(\"mode\", \"FAILFAST\")\\\n",
    "            .load(\"flight_data.csv\")\n",
    "flight_df_header.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df_header.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df_header_schema = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferschema\", \"true\")\\\n",
    "            .option(\"mode\", \"FAILFAST\")\\\n",
    "            .load(\"flight_data.csv\")\n",
    "flight_df_header_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df_header_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_schema = StructType([\n",
    "                        StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "                        StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "                        StructField(\"count\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df_1 = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferschema\", \"false\")\\\n",
    "            .schema(my_schema)\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .load(\"flight_data.csv\")\n",
    "flight_df_1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 6951-7C20\n",
      "\n",
      " Directory of C:\\Users\\HP\\Pyspark\n",
      "\n",
      "12/02/2023  08:56 AM    <DIR>          .\n",
      "12/02/2023  08:56 AM    <DIR>          ..\n",
      "12/01/2023  08:27 PM    <DIR>          .ipynb_checkpoints\n",
      "12/01/2023  08:59 PM               220 corrupted_json.json\n",
      "11/26/2023  06:42 PM               230 employee.csv\n",
      "11/25/2023  11:21 PM             7,121 flight_data.csv\n",
      "12/01/2023  08:37 PM               223 line_delimited_json.json\n",
      "12/01/2023  08:47 PM               232 line_with_extra fields.json\n",
      "12/01/2023  08:57 PM               310 Multi_line_correct.json\n",
      "12/01/2023  08:57 PM               304 Multi_line_incorrect.json\n",
      "10/15/2023  09:15 PM             5,561 Pyspark - Krish Naik's Tutorial.ipynb\n",
      "12/02/2023  08:56 AM             9,164 Pyspark notes.ipynb\n",
      "10/15/2023  10:57 AM               143 test1.csv\n",
      "10/15/2023  10:58 AM               180 test2.csv\n",
      "10/15/2023  10:58 AM               259 test3.csv\n",
      "10/15/2023  10:58 AM             8,188 tips.csv\n",
      "              13 File(s)         32,135 bytes\n",
      "               3 Dir(s)  107,008,069,632 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls C:\\Users\\HP\\Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+\n",
      "| id|    name|age|salary|     address| nominee|\n",
      "+---+--------+---+------+------------+--------+\n",
      "|  1|  Manish| 26| 75000|       bihar|nominee1|\n",
      "|  2|  Nikita| 23|100000|uttarpradesh|nominee2|\n",
      "|  3|  Pritam| 22|150000|   Bangalore|   India|\n",
      "|  4|Prantosh| 17|200000|     Kolkata|   India|\n",
      "|  5|  Vikash| 31|300000|        null|nominee5|\n",
      "+---+--------+---+------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferschema\", \"false\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .load(\"employee.csv\")\n",
    "employee_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_schema = StructType([\n",
    "                        StructField(\"id\", IntegerType(), True),\n",
    "                        StructField(\"name\", StringType(), True),\n",
    "                        StructField(\"age\", IntegerType(), True),\n",
    "                        StructField(\"salary\", IntegerType(), True),\n",
    "                        StructField(\"address\", StringType(), True),\n",
    "                        StructField(\"nominee\", StringType(), True),\n",
    "                        StructField(\"corrupted_records\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-----------------+\n",
      "|id |name    |age|salary|address     |nominee |corrupted_records|\n",
      "+---+--------+---+------+------------+--------+-----------------+\n",
      "|1  |Manish  |26 |75000 |bihar       |nominee1|null             |\n",
      "|2  |Nikita  |23 |100000|uttarpradesh|nominee2|null             |\n",
      "|3  |Pritam  |22 |150000|Bangalore   |India   |nominee3         |\n",
      "|4  |Prantosh|17 |200000|Kolkata     |India   |nominee4         |\n",
      "|5  |Vikash  |31 |300000|null        |nominee5|null             |\n",
      "+---+--------+---+------+------------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .schema(employee_schema)\\\n",
    "            .load(\"employee.csv\")\n",
    "employee_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data = employee_df.filter(col(\"corrupted_records\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+---------+-------+-----------------+\n",
      "| id|    name|age|salary|  address|nominee|corrupted_records|\n",
      "+---+--------+---+------+---------+-------+-----------------+\n",
      "|  3|  Pritam| 22|150000|Bangalore|  India|         nominee3|\n",
      "|  4|Prantosh| 17|200000|  Kolkata|  India|         nominee4|\n",
      "+---+--------+---+------+---------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bad_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = employee_df.filter(col(\"corrupted_records\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+-----------------+\n",
      "| id|  name|age|salary|     address| nominee|corrupted_records|\n",
      "+---+------+---+------+------------+--------+-----------------+\n",
      "|  1|Manish| 26| 75000|       bihar|nominee1|             null|\n",
      "|  2|Nikita| 23|100000|uttarpradesh|nominee2|             null|\n",
      "|  5|Vikash| 31|300000|        null|nominee5|             null|\n",
      "+---+------+---+------+------------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|    name|salary|\n",
      "+---+--------+------+\n",
      "| 20|  Manish| 20000|\n",
      "| 25|  Nikita| 21000|\n",
      "| 16|  Pritam| 22000|\n",
      "| 35|Prantosh| 25000|\n",
      "| 67|  Vikash| 40000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .load(\"line_delimited_json.json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+------+\n",
      "|age|gender|    name|salary|\n",
      "+---+------+--------+------+\n",
      "| 20|  null|  Manish| 20000|\n",
      "| 25|  null|  Nikita| 21000|\n",
      "| 16|  null|  Pritam| 22000|\n",
      "| 35|  null|Prantosh| 25000|\n",
      "| 67|     M|  Vikash| 40000|\n",
      "+---+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .load(\"line_with_extra fields.json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|    name|salary|\n",
      "+---+--------+------+\n",
      "| 20|  Manish| 20000|\n",
      "| 25|  Nikita| 21000|\n",
      "| 16|  Pritam| 22000|\n",
      "| 35|Prantosh| 25000|\n",
      "| 67|  Vikash| 40000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"multiline\", \"true\")\\\n",
    "            .load(\"Multi_line_correct.json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|age|  name|salary|\n",
      "+---+------+------+\n",
      "| 20|Manish| 20000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"multiline\", \"true\")\\\n",
    "            .load(\"Multi_line_incorrect.json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----+--------+------+\n",
      "|_corrupt_record                         |age |name    |salary|\n",
      "+----------------------------------------+----+--------+------+\n",
      "|null                                    |20  |Manish  |20000 |\n",
      "|null                                    |25  |Nikita  |21000 |\n",
      "|null                                    |16  |Pritam  |22000 |\n",
      "|null                                    |35  |Prantosh|25000 |\n",
      "|{\"name\":\"Vikash\",\"age\":67,\"salary\":40000|null|null    |null  |\n",
      "+----------------------------------------+----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .load(\"corrupted_json.json\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "jarvis",
   "language": "python",
   "name": "jarvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
